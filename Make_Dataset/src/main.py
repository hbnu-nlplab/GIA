"""
ë„¤íŠ¸ì›Œí¬ Q&A ë°ì´í„°ì…‹ ìƒì„± (LLM ë¯¸ì‚¬ìš©, ìˆœìˆ˜ ê·œì¹™ ê¸°ë°˜)

NOTE:
    Agent ê¸°ë°˜ ëª¨ë“ˆ(`Make_Dataset/src/agents/*`)ì€ í˜„ì¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    í–¥í›„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ë“±ì´ í•„ìš”í•  ë•Œë¥¼ ëŒ€ë¹„í•´ ë³´ì¡´ë§Œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
"""

import argparse
import json
import random
import csv
import re
from collections import defaultdict
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple

from core.parser import UniversalParser
from core.rule_based_generator import RuleBasedGenerator, RuleBasedGeneratorConfig
from core.builder_core import BuilderCore


def _get_all_categories(policies_path: str) -> List[str]:
    """policies.jsonì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
    with open(policies_path, 'r', encoding='utf-8') as f:
        policies_data = json.load(f)

    categories = set()
    for policy in policies_data.get("policies", []):
        category = policy.get("category")
        if category:
            categories.add(category)

    return sorted(list(categories))


def _normalize_to_text(value: Any) -> str:
    """ê°„ë‹¨í•œ í‰ë¬¸í™”: dict/list/ê¸°íƒ€ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if value is None:
        return "ì •ë³´ ì—†ìŒ"

    if isinstance(value, (set, tuple)):
        value = list(value)

    if isinstance(value, list):
        if not value:
            return "ì—†ìŒ"
        try:
            items = sorted(list({str(x) for x in value}))
        except Exception:
            items = [str(x) for x in value]
        return ", ".join(items)
    if isinstance(value, dict):
        if not value:
            return "ì—†ìŒ"
        try:
            pairs = sorted((str(k), str(v)) for k, v in value.items())
        except Exception:
            pairs = [(str(k), str(v)) for k, v in value.items()]
        return ", ".join([f"{k}={v}" for k, v in pairs])
    if isinstance(value, bool):
        return "true" if value else "false"
    text = str(value)
    return text if text else "ì •ë³´ ì—†ìŒ"


def _split_dataset(items: List[Dict[str, Any]], seed: int = 42, shuffle: bool = False) -> Dict[str, List[Dict[str, Any]]]:
    """ê°„ë‹¨í•œ 8:1:1 ë¶„í•  (ì„ íƒì  ì…”í”Œ)"""
    items_copy = list(items)
    if shuffle:
        random.Random(seed).shuffle(items_copy)
    n = len(items_copy)
    n_train = int(n * 0.8)
    n_val = int(n * 0.1)
    train = items_copy[:n_train]
    val = items_copy[n_train:n_train + n_val]
    test = items_copy[n_train + n_val:]
    return {"train": train, "validation": val, "test": test}


TAG_PREFIX = re.compile(r"^\s*\[[^\]]+\]\s*")


def _strip_tag_prefix(q: str) -> str:
    if not isinstance(q, str):
        return str(q)
    return TAG_PREFIX.sub("", q).strip()


def _build_explanation(metric: str, scope: Dict[str, Any], value: Any) -> str:
    scope_pairs = []
    for key, val in sorted((scope or {}).items()):
        if isinstance(val, (list, dict)):
            scope_pairs.append(f"{key}={json.dumps(val, ensure_ascii=False)}")
        else:
            scope_pairs.append(f"{key}={val}")
    scope_str = ", ".join(scope_pairs) if scope_pairs else "global"
    return f"metric `{metric}` on {scope_str} â†’ {_normalize_to_text(value)}"


ANSWER_TYPE_HINT = {
    "boolean": "true/false (ì†Œë¬¸ì)",
    "numeric": "ì •ìˆ˜ ìˆ«ì",
    "set": "ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í•­ëª© ëª©ë¡ (ì—†ìœ¼ë©´ 'ì—†ìŒ')",
    "list": "ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í•­ëª© ëª©ë¡ (ì—†ìœ¼ë©´ 'ì—†ìŒ')",
    "map": "í‚¤=ê°’ í˜•ì‹ ëª©ë¡ (ì—†ìœ¼ë©´ 'ì—†ìŒ')",
    "text": "ë‹¨ë‹µ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ')"
}


def _ensure_raw_answer(item: Dict[str, Any], raw_value: Any) -> None:
    """ground_truth ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê¸° ì „ ì›ë³¸ ê°’ì„ í•¨ê»˜ ë³´ê´€"""
    if raw_value is None:
        item["ground_truth_raw"] = None
    elif isinstance(raw_value, (list, dict, set, tuple)):
        if isinstance(raw_value, set):
            raw = sorted(list(raw_value))
        elif isinstance(raw_value, tuple):
            raw = list(raw_value)
        else:
            raw = raw_value
        item["ground_truth_raw"] = raw
    else:
        item["ground_truth_raw"] = raw_value


def _write_csv(rows: List[Dict[str, Any]], csv_path: Path) -> None:
    header = [
        "id", "category", "answer_type", "level",
        "question", "ground_truth", "explanation", "source_files"
    ]
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
        w = csv.DictWriter(f, fieldnames=header)
        w.writeheader()
        w.writerows(rows)


def main():
    parser = argparse.ArgumentParser(
        description='ë„¤íŠ¸ì›Œí¬ Q&A ë°ì´í„°ì…‹ ìƒì„± (ê·œì¹™ ê¸°ë°˜, LLM ë¹„ì‚¬ìš©)'
    )

    # ê¸°ë³¸ ì¸ì
    parser.add_argument('--xml-dir', default='data/raw/XML_Data', help='ë„¤íŠ¸ì›Œí¬ ì„¤ì • XML íŒŒì¼ ë””ë ‰í† ë¦¬')
    # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    default_policies = str((Path(__file__).resolve().parents[1] / 'policies.json'))
    parser.add_argument('--policies', default=default_policies, help='ì •ì±… íŒŒì¼ ê²½ë¡œ (JSON)')
    parser.add_argument('--categories', nargs='+', help='ìƒì„±í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ë¯¸ì§€ì • ì‹œ policies.json ì „ì²´)')
    parser.add_argument('--output-dir', default='output/logic_only', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--no-split', action='store_true', help='train/val/test ë¶„í•  ì—†ì´ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥')
    parser.add_argument(
        '--shuffle',
        action='store_true',
        help='train/val/test ë¶„í•  ì‹œ í•­ëª©ì„ ë¬´ì‘ìœ„ë¡œ ì„ìŠµë‹ˆë‹¤ (ê¸°ë³¸: ì •ë ¬ ìˆœì„œ ìœ ì§€)'
    )

    # ìƒì„± ì˜µì…˜
    parser.add_argument('--basic-per-category', type=int, default=0, help='ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ì§ˆë¬¸ ìˆ˜ ì œí•œ(0=ë¬´ì œí•œ)')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥')
    
    # L1 ìƒ˜í”Œë§ ì˜µì…˜
    parser.add_argument('--l1-sample-ratio', type=float, default=0.3,
        help='L1 ë©”íŠ¸ë¦­ì—ì„œ ìƒ˜í”Œë§í•  ì¥ë¹„ ë¹„ìœ¨ (0.0-1.0, ê¸°ë³¸: 0.3)')
    parser.add_argument('--seed', type=int, default=42,
        help='ëœë¤ ì‹œë“œ (ì¬í˜„ì„± ë³´ì¥, ê¸°ë³¸: 42)')

    args = parser.parse_args()

    # ì¹´í…Œê³ ë¦¬ ê²°ì •
    all_categories = _get_all_categories(args.policies)
    target_categories = args.categories or all_categories

    print("=" * 70)
    print("ğŸš€ ë„¤íŠ¸ì›Œí¬ Q&A ë°ì´í„°ì…‹ ìƒì„± (ê·œì¹™ ê¸°ë°˜)")
    print("=" * 70)
    print(f"  * XML directory: {args.xml_dir}")
    print(f"  * Categories: {', '.join(target_categories)}")
    print(f"  * Output directory: {args.output_dir}")
    print(f"  * L1 sample ratio: {args.l1_sample_ratio}")
    print(f"  * Random seed: {args.seed}")
    print("-" * 70)

    try:
        # 1) XML â†’ Facts ë¡œë“œ
        parser_u = UniversalParser()
        facts = parser_u.parse_dir(args.xml_dir)
        if args.verbose:
            print(f"[DEBUG] Loaded devices: {len(facts.get('devices', []))}")

        # 1-1) Facts ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_dir = Path(args.output_dir)
        out_dir.mkdir(parents=True, exist_ok=True)
        facts_file = out_dir / f"facts_{timestamp}.json"
        with open(facts_file, 'w', encoding='utf-8') as f:
            json.dump(facts, f, ensure_ascii=False, indent=2)
        if args.verbose:
            print(f"[DEBUG] Saved facts: {facts_file}")

        # 2) ì •ì±… â†’ DSL ì»´íŒŒì¼ (LLM ë¯¸ì‚¬ìš©)
        rb_cfg = RuleBasedGeneratorConfig(policies_path=args.policies)
        rb = RuleBasedGenerator(rb_cfg)
        dsl = rb.compile(capabilities=facts, categories=target_categories)
        if args.verbose:
            print(f"[DEBUG] DSL items: {len(dsl)}")

        # 3) DSL â†’ ì§ˆë¬¸/ì •ë‹µ í™•ì¥ (BuilderCore)
        core = BuilderCore(facts.get("devices", []))
        by_cat = core.expand_from_dsl(
            dsl, 
            l1_sample_ratio=args.l1_sample_ratio,
            seed=args.seed
        )
        if args.verbose:
            print(f"[DEBUG] L1 sample ratio: {args.l1_sample_ratio}")
            print(f"[DEBUG] Random seed: {args.seed}")

        # 4) í›„ì²˜ë¦¬: ground_truth/explanation/id ì¬êµ¬ì„±
        per_cat: Dict[str, List[Dict[str, Any]]] = {}
        id_counter: defaultdict[Tuple[str, str], int] = defaultdict(int)
        for cat in sorted(by_cat.keys()):
            arr = by_cat[cat]
            keep: List[Dict[str, Any]] = []
            seen_q: set[str] = set()
            arr_sorted = sorted(
                arr,
                key=lambda x: (
                    (x.get("evidence_hint") or {}).get("metric", ""),
                    (x.get("question") or "")
                )
            )
            for t in arr_sorted:
                qa = dict(t)
                evidence = qa.get("evidence_hint") or {}
                metric = evidence.get("metric") or "metric"
                scope = evidence.get("scope") or {}

                if qa.get("question"):
                    qa["question"] = _strip_tag_prefix(qa["question"])

                exp_value = (qa.get("expected_answer") or {}).get("value")
                _ensure_raw_answer(qa, exp_value)
                qa["ground_truth"] = _normalize_to_text(exp_value)
                qa["explanation"] = _build_explanation(metric, scope, exp_value)

                if qa.get("question") and "{host}" in qa["question"]:
                    hosts = []
                    raw_host = scope.get("host")
                    if isinstance(raw_host, str):
                        hosts.append(raw_host)
                    scoped_hosts = scope.get("hosts")
                    if isinstance(scoped_hosts, list):
                        hosts.extend(str(h) for h in scoped_hosts)
                    if not hosts:
                        src_hosts = {Path(f).stem for f in (qa.get("source_files") or []) if f}
                        if src_hosts:
                            hosts = sorted(src_hosts)
                    host_text = ", ".join(hosts) if hosts else "ê´€ë ¨"
                    qa["question"] = qa["question"].replace("{host}", host_text)

                hint = ANSWER_TYPE_HINT.get((qa.get("answer_type") or "").lower())
                if hint and qa.get("question") and "[ë‹µë³€ í˜•ì‹:" not in qa["question"]:
                    qa["question"] = f"{qa['question']}\n[ë‹µë³€ í˜•ì‹: {hint}]"

                qtext = (qa.get("question") or "").strip()
                if qtext in seen_q:
                    continue
                seen_q.add(qtext)

                key = (cat, metric)
                id_counter[key] += 1
                qa["id"] = f"{metric.upper()}_{id_counter[key]:04d}"

                qa.pop("test_id", None)
                qa.pop("origin", None)
                qa.pop("expected_answer", None)

                keep.append(qa)

            if args.basic_per_category and args.basic_per_category > 0:
                keep = keep[: args.basic_per_category]
            per_cat[cat] = keep

        # 5) ì „ì²´ í”Œë«ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•© í›„ ì •ë ¬ ë° ì „ì—­ ì¤‘ë³µ ì œê±°
        all_items: List[Dict[str, Any]] = []
        for cat in sorted(per_cat.keys()):
            all_items.extend(per_cat[cat])
        all_items.sort(
            key=lambda it: (
                it.get("category", ""),
                (it.get("evidence_hint") or {}).get("metric", ""),
                it.get("question", "")
            )
        )
        seen_global: set[str] = set()
        filtered_items: List[Dict[str, Any]] = []
        for it in all_items:
            q = (it.get("question") or "").strip()
            if q in seen_global:
                continue
            seen_global.add(q)
            filtered_items.append(it)
        all_items = filtered_items

        # ë¶„í•  ì˜µì…˜ ì²˜ë¦¬ ë° ì €ì¥
        if args.no_split:
            final_dataset: Any = all_items
            dataset_file = out_dir / f"dataset_logic_only_single_{timestamp}.json"
            with open(dataset_file, 'w', encoding='utf-8') as f:
                json.dump(final_dataset, f, ensure_ascii=False, indent=2)
            total_samples = len(all_items)
        else:
            # ê¸°ë³¸ì€ ì •ë ¬ëœ ìˆœì„œë¥¼ ìœ ì§€í•˜ì§€ë§Œ, í•„ìš” ì‹œ --shuffle ì˜µì…˜ìœ¼ë¡œ ëœë¤ ë¶„í•  ê°€ëŠ¥
            final_dataset = _split_dataset(all_items, shuffle=args.shuffle)
            dataset_file = out_dir / f"dataset_logic_only_{timestamp}.json"
            with open(dataset_file, 'w', encoding='utf-8') as f:
                json.dump(final_dataset, f, ensure_ascii=False, indent=2)
            total_samples = sum(len(final_dataset.get(split, [])) for split in ("train", "validation", "test"))

        # CSV ë³€í™˜ ìˆ˜í–‰
        def _iter_rows(ds: Any) -> List[Dict[str, Any]]:
            items: List[Dict[str, Any]] = []
            if isinstance(ds, dict):
                for split_name in ("train", "validation", "test"):
                    for it in ds.get(split_name, []) or []:
                        items.append(it)
            elif isinstance(ds, list):
                items = ds
            rows: List[Dict[str, Any]] = []
            for it in items:
                rows.append({
                    "id": it.get("id"),
                    "category": it.get("category"),
                    "answer_type": it.get("answer_type"),
                    "level": it.get("level"),
                    "question": it.get("question"),
                    "ground_truth": it.get("ground_truth"),
                    "explanation": it.get("explanation"),
                    "source_files": ", ".join(it.get("source_files") or [])
                })
            return rows

        csv_file = out_dir / f"dataset_logic_only_{timestamp}.csv"
        _write_csv(_iter_rows(final_dataset), csv_file)

        # ìš”ì•½ ì¶œë ¥
        print("\n" + "=" * 70)
        print("âœ… ì™„ë£Œ!")
        print("=" * 70)
        print(f"  â€¢ ì´ ì§ˆë¬¸ ìˆ˜: {total_samples}ê°œ")
        if not args.no_split and isinstance(final_dataset, dict):
            print(f"    - í›ˆë ¨ìš©: {len(final_dataset.get('train', []))}ê°œ")
            print(f"    - ê²€ì¦ìš©: {len(final_dataset.get('validation', []))}ê°œ")
            print(f"    - í…ŒìŠ¤íŠ¸ìš©: {len(final_dataset.get('test', []))}ê°œ")
        print(f"  â€¢ Facts: {facts_file}")
        print(f"  â€¢ Dataset(JSON): {dataset_file}")
        print(f"  â€¢ Dataset(CSV): {csv_file}")

        return 0

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
